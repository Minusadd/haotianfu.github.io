{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Minusadd/haotianfu.github.io/blob/master/irony_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!pip install sklearn\n",
        "!pip install spacytextblob\n",
        "import csv\n",
        "import pandas as pd\n",
        "import random\n",
        "from functools import reduce\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "id": "8Wj4sGUonUVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FILEPATH = \"subreddit_irony_data.csv\""
      ],
      "metadata": {
        "id": "ZK9-zdh7nUMB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load data for baseline and remove nan\n",
        "def load_data():\n",
        "\n",
        "  file = pd.read_csv(FILEPATH)\n",
        "  file.replace(\"nan\",np.nan,inplace=True)\n",
        "  file = file.dropna(subset=['comment', 'label'], axis=0, how='any')\n",
        "  raw_comment = file.loc[:, \"comment\"]\n",
        "  labels = file.loc[:, \"label\"]\n",
        "\n",
        "  return file, list(raw_comment), list(labels)\n"
      ],
      "metadata": {
        "id": "vdaz5plmnT8g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokenizer import Tokenizer\n",
        "\n",
        "def preprocessing_part(raw_tweets):\n",
        "  \n",
        "  # parse all the comments with spacy\n",
        "\n",
        "  cache_path = \"raw_comments.pickle\"\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  if os.path.exists(cache_path): \n",
        "    print(\"Loading parses from cache at %s\"%cache_path)\n",
        "    parsed_tweets = pickle.load(open(cache_path, 'rb'))\n",
        "  else:\n",
        "    parsed_tweets = []\n",
        "    for i,r in enumerate(raw_tweets):\n",
        "      if i % 1000 == 0:\n",
        "        print(\"Processed %d out of %d\"%(i, len(raw_tweets)))\n",
        "      #print(r)\n",
        "      parsed_tweets.append(nlp(r))\n",
        "    if cache_path is not None:\n",
        "       pickle.dump(parsed_tweets, open(cache_path, 'wb'))\n",
        "  new = []\n",
        "  cot = []\n",
        "  #simple preprocessing\n",
        "  for r in parsed_tweets:\n",
        "    new1= []\n",
        "    for token in r:\n",
        "      token = nlp(token.lemma_)\n",
        "      token = token[0].norm_.lower()\n",
        "      \n",
        "      new1.append(token)\n",
        "      cot.append(token)\n",
        "\n",
        "    new.append(new1)\n",
        "  \n",
        "  return new\n"
      ],
      "metadata": {
        "id": "PU7YkT_7TH7Q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#uni-gram and bi-gram features extraction\n",
        "def featurize_part(preproc_X, dv=None, isTest = False):\n",
        "\n",
        "  vectorizer = CountVectorizer(ngram_range=(1,2))\n",
        "  return vectorizer.fit_transform(preproc_X)"
      ],
      "metadata": {
        "id": "0zf3XWpUVfiO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import recall_score, precision_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "def run_kfold_crossval1(X, y, k=5, feature_fn=featurize_part):\n",
        "  acc = []\n",
        "  recall_scores = []\n",
        "  precision_scores = []\n",
        "  X = feature_fn(X)\n",
        "  for i in range(k):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=42) #split data\n",
        "    svm = SGDClassifier(loss=\"log\", penalty=\"l2\", alpha=.01, class_weight='balanced')\n",
        "    parameters = {'alpha':[0.001, .01,  .1, 1., 0.0001, 0.05, 0.005, 0.5]}\n",
        "    clf = GridSearchCV(svm, parameters, scoring='f1') #hyperparameter search\n",
        "    clf = clf.fit(X_train, y_train)\n",
        "    recall_scores.append(recall_score(y_test, clf.predict(X_test)))\n",
        "    precision_scores.append(precision_score(y_test, clf.predict(X_test)))\n",
        "  return recall_scores, precision_scores, clf"
      ],
      "metadata": {
        "id": "TZGA491aV-Es"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import describe\n",
        "from math import sqrt\n",
        "\n",
        "def make_plots(perfs, names):\n",
        "  means = []\n",
        "  stds = []\n",
        "  for i,perf in enumerate(perfs):\n",
        "   n, minmax, mean, var, skew, kurt = describe(perf)\n",
        "   means.append(mean)\n",
        "   stds.append(sqrt(var))\n",
        "   print(\"%s:\\t%.03f\"%(names[i], mean))\n",
        "  plt.bar(np.arange(len(means)), means, yerr=stds)\n",
        "  plt.xticks(np.arange(len(names)), names)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "OVgIxCjwXpN5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import spacy\n",
        "from spacytextblob.spacytextblob import SpacyTextBlob\n",
        "#load data for fully-featurized model\n",
        "def load_data2():\n",
        "  file = pd.read_csv(FILEPATH)\n",
        "  raw_comment = file.loc[:, \"comment\"]\n",
        "  labels = file.loc[:, \"label\"]\n",
        "  threads = file.loc[:, \"thread_title\"]\n",
        "\n",
        "\n",
        "  return file, list(raw_comment), list(labels), list(threads)\n",
        "\n",
        "\n",
        "def preprocessing_part1(raw_tweets):\n",
        "  # parse all the comments with spacy and calculate sentiments\n",
        "\n",
        "  cache_path = \"raw_comments_nnp.pickle\"\n",
        "  cache_path2 = \"raw_comments_sentiment.pickle\"\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  nlp.add_pipe('spacytextblob')\n",
        "  if os.path.exists(cache_path) and os.path.exists(cache_path2): \n",
        "    print(\"Loading parses from cache at %s\"%cache_path)\n",
        "    parsed_tweets = pickle.load(open(cache_path, 'rb'))\n",
        "    sentiments = pickle.load(open(cache_path2, 'rb'))\n",
        "  else:\n",
        "    parsed_tweets = []\n",
        "    sentiments = []\n",
        "    for i,r in enumerate(raw_tweets):\n",
        "      if i % 1000 == 0:\n",
        "        print(\"Processed %d out of %d\"%(i, len(raw_tweets)))\n",
        "      temp = nlp(r)\n",
        "      parsed_tweets.append(temp)\n",
        "      sentiments.append(temp._.blob.polarity>0) #sentiment calculation\n",
        "    if cache_path is not None:\n",
        "       pickle.dump(parsed_tweets, open(cache_path, 'wb'))\n",
        "       pickle.dump(sentiments, open(cache_path2, 'wb'))\n",
        "  new = []\n",
        "  #simple preprocessing\n",
        "  for r in parsed_tweets:\n",
        "    new1= []\n",
        "    for token in r:\n",
        "      if token.is_stop or token.is_punct or token.is_space:\n",
        "        continue\n",
        "      if token.tag_ != 'NNP': #extract NNPs\n",
        "        continue  \n",
        "      else:\n",
        "        if token.like_num:\n",
        "          token = str(\"NUM\")  \n",
        "        else:\n",
        "          token = nlp(token.lemma_)\n",
        "          token = token[0].norm_.lower()\n",
        "      \n",
        "        new1.append(token)\n",
        "      \n",
        "      new1.append(token)\n",
        "\n",
        "    new.append(new1)\n",
        "\n",
        "  return new, sentiments\n",
        "\n",
        "def preprocessing_part2(raw_tweets):\n",
        "  \n",
        "  # parse all the thread titles with spacy\n",
        "  cache_path = \"raw_threads_nnp.pickle\"\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  if os.path.exists(cache_path): \n",
        "    print(\"Loading parses from cache at %s\"%cache_path)\n",
        "    parsed_tweets = pickle.load(open(cache_path, 'rb'))\n",
        "  else:\n",
        "    parsed_tweets = []\n",
        "    for i,r in enumerate(raw_tweets):\n",
        "      if i % 1000 == 0:\n",
        "        print(\"Processed %d out of %d\"%(i, len(raw_tweets)))\n",
        "      parsed_tweets.append(nlp(r))\n",
        "    if cache_path is not None:\n",
        "       pickle.dump(parsed_tweets, open(cache_path, 'wb'))\n",
        "  new = []\n",
        "  # simple preprocessing\n",
        "  for r in parsed_tweets:\n",
        "    new1= []\n",
        "    for token in r:\n",
        "      if token.is_stop or token.is_punct or token.is_space:\n",
        "        continue\n",
        "      if token.tag_ != 'NNP':\n",
        "        continue\n",
        "      else:\n",
        "        if token.like_num:\n",
        "          token = str(\"NUM\")  \n",
        "        else:\n",
        "          token = nlp(token.lemma_)\n",
        "          token = token[0].norm_.lower()\n",
        "      \n",
        "        new1.append(token)\n",
        "\n",
        "    new.append(new1)\n",
        "\n",
        "  return new  "
      ],
      "metadata": {
        "id": "tkBpm2zwJch6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#iteratively find the parent comments of a given comment and concatenate all of them\n",
        "def find_comments(file, comments, full_comments, n):\n",
        "  comments = comments + full_comments[n]\n",
        "  if not np.isnan(file.loc[n,'parent_id']):\n",
        "    parent_index = file[file.comment_id==file.loc[n,'parent_id']].index.tolist()\n",
        "    for i in range(len(parent_index)):\n",
        "      comments = find_comments(file, comments, full_comments, parent_index[i])\n",
        "  return comments\n",
        "#feature extraction of the interaction term and sentiments\n",
        "def featurize_part2(file, X, comments, threads, sentiments, dv=None, isTest = False):\n",
        "  \n",
        "  dicts = []\n",
        "  words = []\n",
        "  #bag of NNPs\n",
        "  for i in X:\n",
        "    words.extend(comments[i])\n",
        "\n",
        "  for i in X:\n",
        "    words.extend(threads[i])\n",
        "  words = sorted(list(set(words)))\n",
        "  words_ = [\"\".join([val, \"{}\".format(i)]) for val in words for i in range(4)]\n",
        "  #add another feature for sentiments\n",
        "  words_.append('sentiment1')\n",
        "  step = 0\n",
        "  #interaction term of NNP + subreddit + sentiment\n",
        "  for j in X:\n",
        "    if not np.isnan(file.loc[j,'label']): #remove nan\n",
        "      full_comments_w = find_comments(file, [], comments, j)\n",
        "      full_comments_w = full_comments_w + threads[j]\n",
        "      bow = np.zeros(4*len(words)+1)\n",
        "      for w in full_comments_w:\n",
        "        for i, w1 in enumerate(words):\n",
        "          if w==w1:\n",
        "            if sentiments[j] == 0 and file.loc[j, 'subreddit'] == 'progressive':\n",
        "              bow[i*4] += 1\n",
        "            elif sentiments[j] == 0 and file.loc[j, 'subreddit'] == 'Conservative':\n",
        "              bow[i*4+1] += 1\n",
        "            elif sentiments[j] == 1 and file.loc[j, 'subreddit'] == 'progressive':\n",
        "              bow[i*4+2] += 1\n",
        "            elif sentiments[j] == 1 and file.loc[j, 'subreddit'] == 'Conservative':\n",
        "              bow[i*4+3] += 1\n",
        "      bow[-1] = sentiments[j]\n",
        "      ow = dict(zip(words_, bow))\n",
        "      dicts.append(ow)\n",
        "      step += 1\n",
        "  #Dictvectorizer\n",
        "  if isTest is False:\n",
        "    dv = DictVectorizer()\n",
        "    X = dv.fit_transform(dicts)\n",
        "    return X, dv\n",
        "  else:\n",
        "    return dv.transform(dicts), dv"
      ],
      "metadata": {
        "id": "xRDud9gpicj0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: List<List<String>> X, Vector<Int> y\n",
        "# Output: List<Float> accuracies\n",
        "def run_kfold_crossval2(comments, threads, y, file, sentiments, k=5, feature_fn=featurize_part2):\n",
        "  acc = []\n",
        "  recall_scores = []\n",
        "  precision_scores = []\n",
        "  X = list(range(len(y)))\n",
        "  for i in range(k):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=42)\n",
        "    X_train, dv = feature_fn(file, X_train, comments, threads, sentiments)\n",
        "    X_test, dv = feature_fn(file, X_test, comments, threads, sentiments, dv=dv, isTest=True)\n",
        "    y_train = np.array(y_train)[~np.isnan(y_train)].tolist() #remove nan\n",
        "    y_test = np.array(y_test)[~np.isnan(y_test)].tolist() #remove nan\n",
        "    svm = SGDClassifier(loss=\"log\", penalty=\"l2\", alpha=.01, class_weight='balanced')\n",
        "    parameters = {'alpha':[0.001, 0.005, 0.0001, .01, 0.05, 0.5, .1]} #hyperparameter search\n",
        "    clf = GridSearchCV(svm, parameters, scoring='f1')\n",
        "    clf = clf.fit(X_train, y_train)\n",
        "    recall_scores.append(recall_score(y_test, clf.predict(X_test)))\n",
        "    precision_scores.append(precision_score(y_test, clf.predict(X_test)))\n",
        "    print(\"recall:\", recall_score(y_test, clf.predict(X_test)))\n",
        "    print(\"precision:\", precision_score(y_test, clf.predict(X_test)))\n",
        "\n",
        "  return recall_scores, precision_scores, clf\n"
      ],
      "metadata": {
        "id": "OBgmP3vtXIJw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "58_BpQzNqUWT"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "  file, comments, labels = load_data()\n",
        "  X_preproc = preprocessing_part(comments)\n",
        "  recall_score1, precision_score1, clf = run_kfold_crossval1(comments, labels, k=100, feature_fn=featurize_part) \n",
        "  bow_mean_precision, bow_median_precision, bow_25th_perc_precision, bow_75th_perc_precision = np.mean(precision_score1), np.quantile(precision_score1, 0.5), np.quantile(precision_score1, 0.25), np.quantile(precision_score1, 0.75)\n",
        "  bow_mean_recall, bow_median_recall, bow_25th_perc_recall, bow_75th_perc_recall = np.mean(recall_score1), np.quantile(recall_score1, 0.5), np.quantile(recall_score1, 0.25), np.quantile(recall_score1, 0.75)\n",
        "  print(\"mean_precision:\", bow_mean_precision)\n",
        "  print(\"mean_recall:\", bow_mean_recall)\n",
        "  file, raw_comment, labels, threads = load_data2()\n",
        "  comments, sentiments = preprocessing_part1(raw_comment)\n",
        "  threads = preprocessing_part2(threads)\n",
        "  recall_score2, precision_score2, clf_ = run_kfold_crossval2(comments, threads, labels, file, sentiments, k=50, feature_fn=featurize_part2)\n",
        "  np_mean_precision, np_median_precision, np_25th_perc_precision, np_75th_perc_precision = np.mean(precision_score2), np.quantile(precision_score2, 0.5), np.quantile(precision_score2, 0.25), np.quantile(precision_score2, 0.75)\n",
        "  np_mean_recall, np_median_recall, np_25th_perc_recall, np_75th_perc_recall = np.mean(recall_score2), np.quantile(recall_score2, 0.5), np.quantile(recall_score2, 0.25), np.quantile(recall_score2, 0.75)\n",
        "\n",
        "  \n",
        "  #do not edit below this line\n",
        "\n",
        "  def fformat(f):\n",
        "    return \"%.2f\" % f\n",
        "\n",
        "  print(\"Bag of Words Baseline\")\n",
        "  print(\"Precision\")\n",
        "  print(fformat(bow_mean_precision), fformat(bow_median_precision), fformat(bow_25th_perc_precision), fformat(bow_75th_perc_precision))\n",
        "  print(\"Recall\")\n",
        "  print(fformat(bow_mean_recall), fformat(bow_median_recall), fformat(bow_25th_perc_recall), fformat(bow_75th_perc_recall))\n",
        "\n",
        "  print(\"NP Sentiment Context Model\")\n",
        "  print(\"Precision\")\n",
        "  print(fformat(np_mean_precision), fformat(np_median_precision), fformat(np_25th_perc_precision), fformat(np_75th_perc_precision))\n",
        "  print(\"Recall\")\n",
        "  print(fformat(np_mean_recall), fformat(np_median_recall), fformat(np_25th_perc_recall), fformat(np_75th_perc_recall))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XrRf2oGymQ7",
        "outputId": "9d20d9c7-c9ea-44ce-9c2b-6f16ee836f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading parses from cache at raw_comments.pickle\n",
            "mean_precision: 0.08763273856175895\n",
            "mean_recall: 0.13825\n",
            "Loading parses from cache at raw_comments_nnp.pickle\n",
            "Loading parses from cache at raw_threads_nnp.pickle\n",
            "recall: 0.34285714285714286\n",
            "precision: 0.1875\n",
            "recall: 0.37142857142857144\n",
            "precision: 0.1625\n",
            "recall: 0.37142857142857144\n",
            "precision: 0.19696969696969696\n",
            "recall: 0.37142857142857144\n",
            "precision: 0.19117647058823528\n",
            "recall: 0.42857142857142855\n",
            "precision: 0.17857142857142858\n",
            "recall: 0.42857142857142855\n",
            "precision: 0.24193548387096775\n",
            "recall: 0.37142857142857144\n",
            "precision: 0.22413793103448276\n",
            "recall: 0.37142857142857144\n",
            "precision: 0.18309859154929578\n",
            "recall: 0.4\n",
            "precision: 0.175\n",
            "recall: 0.34285714285714286\n",
            "precision: 0.21052631578947367\n",
            "recall: 0.34285714285714286\n",
            "precision: 0.21818181818181817\n",
            "recall: 0.37142857142857144\n",
            "precision: 0.20634920634920634\n",
            "recall: 0.34285714285714286\n",
            "precision: 0.18461538461538463\n",
            "recall: 0.37142857142857144\n",
            "precision: 0.20634920634920634\n",
            "recall: 0.4\n",
            "precision: 0.21875\n",
            "recall: 0.34285714285714286\n",
            "precision: 0.1875\n",
            "recall: 0.4\n",
            "precision: 0.16666666666666666\n",
            "recall: 0.37142857142857144\n",
            "precision: 0.1625\n",
            "recall: 0.42857142857142855\n",
            "precision: 0.15625\n",
            "recall: 0.37142857142857144\n",
            "precision: 0.22033898305084745\n",
            "recall: 0.3142857142857143\n",
            "precision: 0.1864406779661017\n",
            "recall: 0.34285714285714286\n",
            "precision: 0.23076923076923078\n",
            "recall: 0.4\n",
            "precision: 0.15053763440860216\n",
            "recall: 0.37142857142857144\n",
            "precision: 0.22033898305084745\n",
            "recall: 0.37142857142857144\n",
            "precision: 0.1625\n",
            "recall: 0.37142857142857144\n",
            "precision: 0.1780821917808219\n",
            "recall: 0.37142857142857144\n",
            "precision: 0.18055555555555555\n",
            "recall: 0.4\n",
            "precision: 0.1794871794871795\n",
            "recall: 0.37142857142857144\n",
            "precision: 0.2\n",
            "recall: 0.34285714285714286\n",
            "precision: 0.18181818181818182\n",
            "recall: 0.34285714285714286\n",
            "precision: 0.18181818181818182\n",
            "recall: 0.37142857142857144\n",
            "precision: 0.24074074074074073\n",
            "recall: 0.34285714285714286\n",
            "precision: 0.24\n"
          ]
        }
      ]
    }
  ]
}